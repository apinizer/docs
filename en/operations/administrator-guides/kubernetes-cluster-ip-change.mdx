---
title: "IP Change in Kubernetes Cluster - Virtual IP Usage"
description: "You can provide uninterrupted service by routing to virtual IP without breaking existing Kubernetes cluster, configure it to work with high availability, and configure endpoint address as virtual IP with kubeadm init command."
---

## Overview

In this document, we will ensure our existing Kubernetes cluster works with high availability along with uninterrupted service by routing to virtual IP without breaking it.

**Existing Machines:**

- Kubernetes Master1
- Kubernetes Master2
- Kubernetes Master3
- Kubernetes Worker1
- Kubernetes Worker2

## Existing System and Virtual IP Access

In the existing system, a Kubernetes cluster started with **kubeadm init** command on **Master1** was created and then other machines (Master2, Master3, Worker1, and Worker2) were included in this cluster with **worker** role.

By defining a **Virtual IP** on Load Balancer, we need to access existing machines to this **Virtual IP** from **6443** port.

<Warning>
If you are not using a load balancer and cannot create a virtual IP on the network, [click here](/tr/setup/kubernetes/kubernetes-high-availability-yuksek-erisim-cluster.mdx) to perform this operation with **Keepalived** and **HAProxy** tools you can use for this.

**Keepalived** is used to create virtual IP, **HAProxy** is used for load balancing operation.
</Warning>

## IP Change

When there are multiple master nodes in the cluster, we need to reconfigure the cluster to leave these masters as a single master.

This operation usually starts by completely removing other masters from the cluster.

### Removing Master Nodes from Cluster

The following command is run for other masters (**master2 and master3**) **except master1**.

```bash
sudo kubeadm reset
```

Other masters (master2 and master3) are deleted from the cluster from Master1 server.

```bash
kubectl delete nodes master2
kubectl delete nodes master3
```

<Info>
Master1 and worker servers should remain in the existing system.
</Info>

### Tasks to be Done on Master1 Server

<Steps>
  <Step>
    **Stopping Services**

    Kubelet and containerd (and docker application if docker is used) are stopped.

    ```bash
    sudo systemctl stop kubelet
    sudo systemctl stop containerd
    ```
  </Step>
  <Step>
    **File Backup and Cleaning**

    Backups of some files are taken and deleted.

    ```bash
    sudo mv -f /etc/kubernetes /etc/kubernetes.backup
    sudo mv -f /var/lib/kubelet /var/lib/kubelet.backup
    sudo mkdir -p /etc/kubernetes/pki
    sudo cp -r /etc/kubernetes_backup/pki /etc/kubernetes
    sudo rm -rf /etc/kubernetes/pki/{apiserver.*,etcd/peer.*}
    sudo rm -f ~/.kube/config
    ```
  </Step>
  <Step>
    **Starting Containerd**

    Containerd (and docker application if docker is used) is started.

    ```bash
    sudo systemctl start containerd
    ```
  </Step>
  <Step>
    **Virtual IP Configuration with Kubeadm Init**

    Kubeadm init command is run again by editing endpoint address and used as **Virtual IP**.

    ```bash
    sudo kubeadm init --pod-network-cidr "10.244.0.0/16" --control-plane-endpoint <VIRTUAL_IP> --upload-certs --ignore-preflight-errors=DirAvailable--var-lib-etcd
    ```

    <Warning>
    Kubeadm **join** commands are noted.
    </Warning>
  </Step>
</Steps>

### Tasks for Worker1 and Worker2

<Steps>
  <Step>
    **Stopping Services**

    Kubelet and containerd (and docker application if docker is used) are stopped.

    ```bash
    sudo systemctl stop kubelet
    sudo systemctl stop containerd
    ```
  </Step>
  <Step>
    **File Backup**

    Backups of some files are taken.

    ```bash
    sudo mv -f /etc/kubernetes /etc/kubernetes.backup
    sudo mv -f /var/lib/kubelet /var/lib/kubelet.backup
    ```
  </Step>
  <Step>
    **Starting Services**

    Containerd (and docker application if docker is used) and kubelet are started.

    ```bash
    sudo systemctl start kubelet
    sudo systemctl start containerd
    ```
  </Step>
  <Step>
    **Worker Join**

    Kubeadm **worker join** command is run on **worker1** and **worker2** machines.
  </Step>
</Steps>

### Tasks for Master2 and Master3

In this step, it will be sufficient to just run the **master join** command on master2 and master3.

### Cluster Status Check

You can see your new cluster information with the following commands.

```bash
kubectl cluster-info
kubectl get node -o wide
```

