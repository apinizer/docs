---
title: "Benchmark Results"
description: "Apinizer platform performance test results and load test details. With 2,000 concurrent threads on a 1 CPU 8 Core system, 15,000 reqs/sec is achieved. Test results vary depending on the response time of the called service, network latency, and system requirements of policies added to the gateway."
---

## Summary

<Info>
  **With 2,000 Concurrent Threads on a 1 CPU 8 Core system, 15,000 reqs/sec is achieved.**
</Info>

Since these results vary depending on the response time of the called service, network latency, and system requirements of policies added to the gateway, you can review the details of our load test in the section below.

## Test Environment

DigitalOcean platform was used as infrastructure for running load tests due to Apinizer platform's ease of use and quick support.

### Load Test Topology

The load test topology is as follows:

<img src="/images/develop/benchmark/yuk-testi-topolojisi.png" alt="Load Test Topology" width="807" height="536" />

<CardGroup cols={2}>
  <Card title="Load Test Server" icon="server">
    * CPU-Optimized
    * Dedicated CPU
    * 8 vCPUs (Intel Xeon Scalable, 2.5 GHz)
    * 16 GB RAM
    * 100 GB Disk
    * CentOS 8.3 x64
  </Card>
  <Card title="Kubernetes Master & MongoDB" icon="database">
    * CPU-Optimized
    * Dedicated CPU
    * 4 vCPUs (Intel Xeon Scalable, 2.5 GHz)
    * 8 GB RAM
    * 50 GB Disk
    * CentOS 8.3 x64
  </Card>
  <Card title="Elasticsearch" icon="search">
    * CPU-Optimized
    * Dedicated CPU
    * 8 vCPUs (Intel Xeon Scalable, 2.5 GHz)
    * 16 GB RAM
    * 100 GB Disk
    * CentOS 8.3 x64
  </Card>
  <Card title="Kubernetes Worker" icon="server">
    * CPU-Optimized
    * Dedicated CPU
    * 16 vCPUs (Intel Xeon Scalable, 2.5 GHz)
    * 32 GB RAM
    * 200 GB Disk
    * CentOS 8.3 x64
  </Card>
</CardGroup>

## Test Setup

### 1. Load Test Server Setup and JMeter Configuration

JMeter was used for load testing. Setup steps:

<AccordionGroup>
  <Accordion title="Java Installation">
    ```bash
    yum install java-1.8.0-openjdk -y
    java -version
    ```
    
    **Installation Verification:**
    ```
    openjdk version "1.8.0_275"
    OpenJDK Runtime Environment (build 1.8.0_275-b01)
    OpenJDK 64-Bit Server VM (build 25.275-b01, mixed mode)
    ```
  </Accordion>
  
  <Accordion title="JMeter Installation">
    ```bash
    yum install wget -y
    wget http://apache.stu.edu.tw//jmeter/binaries/apache-jmeter-5.2.1.tgz
    tar -xf apache-jmeter-5.2.1.tgz
    ```
    
    **Environment Variables:**
    ```bash
    export JMETER_HOME=/root/apache-jmeter-5.2.1
    export PATH=$JMETER_HOME/bin:$PATH
    source ~/.bashrc
    ```
  </Accordion>
  
  <Accordion title="Test Scenario Configuration">
    * Thread count configured parametrically
    * Test duration configured parametrically
    * HTTP requests configured
    * Result collection mechanism set up
  </Accordion>
</AccordionGroup>

### 2. NGINX Configuration

NGINX was used as a load balancer:

<CardGroup cols={2}>
  <Card title="NGINX Features" icon="network-wired">
    * Load balancing
    * SSL termination
    * Reverse proxy
    * Health check
  </Card>
  <Card title="Configuration" icon="gear">
    * Upstream server definitions
    * Proxy pass settings
    * Timeout settings
    * Connection pool settings
  </Card>
</CardGroup>

### 3. Apinizer and Log Server Setup

Kubernetes, MongoDB, and Elasticsearch installations were done according to Apinizer installation documentation.

<Info>
  Kubernetes master, worker, MongoDB, Elasticsearch, and Apinizer installations were done following the installation steps at [Apinizer Installation Documentation](/en/setup/overview).
</Info>

## Important Points of Load Testing

Points to consider when testing:

<CardGroup cols={2}>
  <Card title="Asynchronous Logging" icon="file-lines">
    Apinizer stores all request & response messages and metrics asynchronously in the Elasticsearch log database. During tests, these logging operations continued as they should.
  </Card>
  <Card title="Network Latency" icon="network-wired">
    Internal IPs were used in all our tests to reduce network latency and see Apinizer's real impact.
  </Card>
  <Card title="Pod Restart" icon="refresh">
    We particularly observed that Kubernetes did not restart pods during runtime. Restart count is an important parameter as it reflects overload/congestion or error conditions.
  </Card>
  <Card title="JVM Monitoring" icon="chart-line">
    JVM performance metrics were monitored with JConsole. JMX service was exposed to the external world via Kubernetes.
  </Card>
</CardGroup>

## Test Scenarios

Test scenarios were configured for different conditions:

<AccordionGroup>
  <Accordion title="Condition A - Basic Configuration">
    * Minimal configuration
    * Basic API Proxy
    * Test without policies
  </Accordion>
  
  <Accordion title="Condition B - Medium Level Configuration">
    * Medium level configuration
    * Basic policies
    * Standard usage scenario
  </Accordion>
  
  <Accordion title="Condition C - Advanced Configuration">
    * Advanced configuration
    * Multiple policy support
    * High performance scenario
  </Accordion>
  
  <Accordion title="Condition D - Optimized Configuration">
    * Optimized configuration
    * Performance optimizations
    * Production-ready scenario
  </Accordion>
</AccordionGroup>

## Load Test Results

### GET Requests Results

| Condition | Thread Count | Throughput (reqs/sec) | Avg Response Time (ms) |
|-----------|--------------|----------------------|------------------------|
| **A** | 50 | 1,133 | 43 |
| | 100 | 1,100 | 90 |
| | 250 | 1,025 | 242 |
| | 500 | 963 | 516 |
| **B** | 50 | 2,232 | 22 |
| | 100 | 2,169 | 45 |
| | 250 | 2,089 | 119 |
| | 500 | 1,915 | 259 |
| | 1,000 | 1,762 | 564 |
| | 1,500 | 1,631 | 915 |
| | 2,000 | 1,379 | 1,441 |
| **C** | 50 | 8,090 | 6 |
| | 100 | 7,816 | 12 |
| | 250 | 7,011 | 35 |
| | 500 | 6,759 | 73 |
| | 1,000 | 6,742 | 147 |
| | 1,500 | 6,683 | 223 |
| | 2,000 | 6,692 | 297 |
| | 4,000 | 6,448 | 617 |
| **D** | 50 | 15,420 | 3 |
| | 100 | 15,812 | 6 |
| | 250 | 15,614 | 15 |
| | 500 | 15,664 | 31 |
| | 1,000 | 15,454 | 64 |
| | 1,500 | 15,026 | 99 |
| | 2,000 | 14,839 | 133 |
| | 4,000 | 14,356 | 276 |
| | 8,000 | 11,603 | 655 |

### POST 5KB Requests Results

| Condition | Thread Count | Throughput (reqs/sec) | Avg Response Time (ms) |
|-----------|--------------|----------------------|------------------------|
| **A** | 50 | 1,002 | 49 |
| | 100 | 983 | 101 |
| | 250 | 852 | 292 |
| **B** | 50 | 1,868 | 26 |
| | 100 | 1,768 | 56 |
| | 250 | 1,456 | 170 |
| | 500 | 1,398 | 355 |
| | 1,000 | 1,229 | 809 |
| | 1,500 | 1,199 | 1,245 |
| **C** | 50 | 7,353 | 6 |
| | 100 | 7,257 | 13 |
| | 250 | 7,138 | 34 |
| | 500 | 7,141 | 69 |
| | 1,000 | 7,011 | 141 |
| | 1,500 | 6,935 | 215 |
| **D** | 50 | 13,396 | 3 |
| | 100 | 13,482 | 7 |
| | 250 | 13,587 | 18 |
| | 500 | 13,611 | 36 |
| | 1,000 | 13,562 | 73 |
| | 1,500 | 13,208 | 112 |
| | 2,000 | 13,179 | 150 |
| | 4,000 | 12,792 | 309 |
| | 8,000 | 11,115 | 701 |

### POST 50KB Requests Results

| Condition | Thread Count | Throughput (reqs/sec) | Avg Response Time (ms) |
|-----------|--------------|----------------------|------------------------|
| **A** | 50 | 675 | 73 |
| | 100 | 653 | 152 |
| | 250 | 554 | 448 |
| **B** | 50 | 1,437 | 34 |
| | 100 | 1,409 | 70 |
| | 250 | 1,223 | 203 |
| | 500 | 1,149 | 432 |
| | 1,000 | 877 | 1,134 |
| **C** | 50 | 4,679 | 10 |
| | 100 | 4,675 | 21 |
| | 250 | 4,020 | 61 |
| | 500 | 3,221 | 154 |
| | 1,000 | 2,962 | 335 |
| **D** | 50 | 4,683 | 10 |
| | 100 | 4,671 | 21 |
| | 250 | 4,382 | 56 |
| | 500 | 3,496 | 142 |
| | 1,000 | 3,046 | 326 |
| | 1,500 | 2,853 | 522 |
| | 2,000 | 2,794 | 710 |

## Interpreting Results

### Concurrent Users and Request Count

<Warning>
  **Important:** A common mistake when examining results is confusing session count with instant request count.
</Warning>

**Throughput & Concurrent Users:**

<img src="/images/develop/benchmark/throughput-concurrent-users.png" alt="Throughput and Concurrent User Count" width="577" height="345" />

**Average Memory Usage:**

<img src="/images/develop/benchmark/avg-memory-usage-concurrent-users.png" alt="Average Memory Usage" width="437" height="300" />

<CardGroup cols={2}>
  <Card title="Request" icon="arrow-right">
    An HTTP request made with a specific HTTP method to a specific target
  </Card>
  <Card title="Session" icon="users">
    Zero or more requests can be made per session
  </Card>
</CardGroup>

Keeping sessions in gateways is very rare, generally service access is stateless. Therefore, measuring concurrent request count and latency becomes more meaningful.

### Scaling

<CardGroup cols={2}>
  <Card title="Vertical Scaling" icon="arrows-up-down">
    When concurrent user count increases, throughput increases up to a certain limit. After that, it starts to decline. This natural course indicates that vertical growth has a limit.
  </Card>
  <Card title="Horizontal Scaling" icon="arrows-left-right">
    To support more concurrent users with acceptable response times, horizontal or vertical scaling should be considered together. Since Kubernetes infrastructure is used in Apinizer, this operation can be configured very easily and quickly.
  </Card>
</CardGroup>

### Message Size Impact

When message sizes increase, processing power increases, so throughput decreases. Therefore, response time also increases.

<AccordionGroup>
  <Accordion title="GET vs POST 5KB">
    Although request sizes are generally around 1KB average in real-life scenarios, we found it worthwhile to examine 5KB and 50KB POST requests since there was a very small difference between our 1KB POST and GET requests in our tests.
  </Accordion>
  
  <Accordion title="POST 5KB vs POST 50KB">
    Although results naturally follow a lower value compared to GET requests, it was pleasing for us that numbers dropped to only one-fourth despite a 10-fold increase in data.
  </Accordion>
</AccordionGroup>

### Memory Usage

RAM usage rates were very consistent throughout the load test. Even when request sizes increased tenfold, no significant increase in RAM usage was observed. This proved that OpenJ9 was the right choice.

## Policy Performance Impact

Each policy we add to the gateway affects performance according to its complexity and dependencies.

### Basic Authentication Policy Test

Tests were performed with "Basic Authentication" policy added (Condition D):

| Thread Count | GET Throughput | GET Avg (ms) | GET with Policy Throughput | GET with Policy Avg (ms) |
|--------------|----------------|--------------|----------------------------|--------------------------|
| 50 | 15,420 | 3 | 14,760 | 3 |
| 100 | 15,812 | 6 | 14,843 | 6 |
| 250 | 15,614 | 15 | 14,891 | 16 |
| 500 | 15,664 | 31 | 14,748 | 33 |
| 1,000 | 15,454 | 64 | 14,285 | 68 |
| 1,500 | 15,026 | 99 | 14,373 | 102 |
| 2,000 | 14,839 | 133 | 14,280 | 136 |
| 4,000 | 14,356 | 276 | 13,795 | 279 |
| 8,000 | 11,603 | 655 | 11,437 | 672 |

**Throughput & Concurrent Users:**

<img src="/images/develop/benchmark/throughput-policy-comparison.png" alt="Throughput Comparison - With and Without Policy" width="577" height="345" />

**Average Memory Usage:**

<img src="/images/develop/benchmark/avg-memory-usage-policy-comparison.png" alt="Average Memory Usage - With and Without Policy" width="437" height="300" />

<Info>
As we can see, there was a performance impact, albeit imperceptible. However, if a computationally expensive policy like "content filtering" were added, or a policy requiring external connections like "LDAP Authentication" that also adds network latency were added, performance would drop even faster.
</Info>

### Policy Selection Best Practices

<CardGroup cols={2}>
  <Card title="Policy Complexity" icon="slider">
    It is important to know how much load each policy will bring and choose the design accordingly.
  </Card>
  <Card title="External Connections" icon="network-wired">
    Policies requiring external connections like LDAP Authentication add network latency and affect performance.
  </Card>
  <Card title="Processing Power" icon="microchip">
    Computationally expensive policies like content filtering increase CPU usage.
  </Card>
  <Card title="Policy Ordering" icon="list-ol">
    Policy ordering and conditional execution affect performance.
  </Card>
</CardGroup>

## Performance Metrics

### Throughput (Processing Speed)

Throughput shows the number of requests processed per second. With optimized configuration in Condition D:

<CardGroup cols={3}>
  <Card title="GET Requests" icon="arrow-right">
    15,000+ reqs/sec (2,000 thread)
  </Card>
  <Card title="POST 5KB Requests" icon="arrow-right">
    13,000+ reqs/sec (2,000 thread)
  </Card>
  <Card title="POST 50KB Requests" icon="arrow-right">
    2,700+ reqs/sec (2,000 thread)
  </Card>
</CardGroup>

### Response Time

Average response times:

<CardGroup cols={3}>
  <Card title="GET Requests" icon="clock">
    3-655 ms (depending on thread count)
  </Card>
  <Card title="POST 5KB Requests" icon="clock">
    3-701 ms
  </Card>
  <Card title="POST 50KB Requests" icon="clock">
    10-710 ms
  </Card>
</CardGroup>

### Scalability

<AccordionGroup>
  <Accordion title="Vertical Scaling">
    * Tested up to 8,000 threads on a single node
    * Optimal performance observed at 2,000 threads
    * Throughput starts to drop at higher thread counts
  </Accordion>
  
  <Accordion title="Horizontal Scaling">
    * Easy scaling with Kubernetes infrastructure
    * Multiple gateway support with load balancer
    * Automatic pod scaling
  </Accordion>
</AccordionGroup>

## Results and Recommendations

### Key Findings

<CardGroup cols={2}>
  <Card title="High Performance" icon="gauge">
    Throughput of 15,000+ reqs/sec achieved with optimized configuration.
  </Card>
  <Card title="Low Latency" icon="clock">
    Response times starting from 3ms for GET requests were observed.
  </Card>
  <Card title="Memory Efficiency" icon="memory">
    Even when message size increased tenfold, no significant increase in RAM usage was observed.
  </Card>
  <Card title="Policy Impact" icon="shield">
    Simple policies like Basic Authentication have minimal performance impact.
  </Card>
</CardGroup>

### Recommendations

<AccordionGroup>
  <Accordion title="Production Environment">
    * Use Condition D configuration
    * Perform horizontal scaling with Kubernetes
    * Configure monitoring and alerting
  </Accordion>
  
  <Accordion title="Policy Management">
    * Evaluate performance impact of policies
    * Remove unnecessary policies
    * Optimize policy ordering
  </Accordion>
  
  <Accordion title="Capacity Planning">
    * Calculate expected load
    * Optimize thread count
    * Plan horizontal scaling
  </Accordion>
</AccordionGroup>

## Next Steps

<CardGroup cols={2}>
  <Card title="Capacity Planning" icon="calculator" href="/en/concepts/deployment/capacity-planning">
    Review capacity planning guide
  </Card>
  <Card title="Capacity Planning" icon="calculator" href="/en/concepts/deployment/capacity-planning">
    Learn hardware requirements
  </Card>
  <Card title="Deployment Topologies" icon="network-wired" href="/en/concepts/deployment/overview">
    Review deployment topologies
  </Card>
  <Card title="Performance Tuning" icon="gauge" href="/en/concepts/deployment/capacity-planning">
    Perform performance optimization
  </Card>
</CardGroup>

